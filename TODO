* add test for compute_ious
* Unfuck docs: use material for mkdocs

* Start with detections: some MVP with simple inputs, then modularize it out


---

Detections:
* add frame getter
  * also `__in__` or whatever it is (contains?)

* computations: calculate_pr_curve should be renamed to evaluate_dataset
  * should return matching and ignored [dets, gts] only, for whole dataset
  * pr_curve can then be a non-numba function
  * confusion matrix also directly takes from it

* add basic compute metrics, we need
  * AP for each class + average across classes
  * PR curve for each class,
  * precision and recall (last one)
  * ???

* Summary:
  * the same as above, but across thresholds etc
* confusion matrix
  * here we just care about matching, and would be global

---

* for tracks hard code field names (do not use common global vars) for reading/writing formats
* Add class names to tracks - maybe change the class list arg to dict? 

---

